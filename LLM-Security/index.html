<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Security of LLM Information Hub</title>
    <link rel="stylesheet" href="styles.css">
    <script src="script.js" defer></script>
</head>
<body>
    <header>
        <h1>Security of LLM Information Hub</h1>
        <nav>
            <h1>Welcome to Our Website!</h1>
            <p>For inquiries, requests, or job proposals, please DM <a href = "https://x.com/7eHnpgIYyHE4iyG">X</a></p>
            
            <p>We are looking for volunteers to help build the site.</p>
            <!-- <ul>
                <li><a href="#attack">Attack</a></li>
                <li><a href="#defense">Defense</a></li>
                <li><a href="#vulnerability-research">LLM Vulnerability Research</a></li>
                <li><a href="#datasets">Datasets</a></li>
            </ul>
        </nav> -->
        <!-- <button onclick="toggleLanguage()">日本語</button> -->
    </header>
    <main>
        <section id="attack">
            <h1>For Biginers</h1>
                <!-- <h4>Attack</h4>
                <h4>Defence</h4>
                <h4>Others</h4>
                <h4>Famous X accounts</h4> -->
                <p>All you need is these papers</p>
                <p>--comming soon--</p>
            <h1>Informartion Hub</h1>
            <h2>Attack</h2>
            <h4>Prompt Injection</h4>
            <ul>
                <li><a href="https://arxiv.org/abs/2306.05499">Prompt Injection attack against LLM-integrated Applications</a></li>
                <li><a href="https://arxiv.org/abs/2302.12173">Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</a></li>
                <li><a href="https://arxiv.org/abs/2211.09527">Ignore Previous Prompt: Attack Techniques For Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2306.08833">Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection</a></li>
                <li><a href="https://arxiv.org/abs/2203.10714">A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement</a></li>
                <li><a href="https://arxiv.org/abs/2302.04237">Black Box Adversarial Prompting for Foundation Models</a></li>
                <li><a href="https://aclanthology.org/2022.acl-long.174/">Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis</a></li>
                <li><a href="https://arxiv.org/pdf/2206.11349">Prompt Injection: Parameterization of Fixed Inputs</a></li>
                <li><a href="https://arxiv.org/abs/2310.12815">Prompt Injection Attacks and Defenses in LLM-Integrated Applications</a></li>
                <li><a href="https://arxiv.org/abs/2308.01990">From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?</a></li>
            </ul>
            <h4>Jailbreak</h4>
                <ul>
                    <li><a href="https://www.digitaltrends.com/computing/how-to-jailbreak-chatgpt/">How to jailbreak ChatGPT: get it to really do what you want</a></li>
                    <li><a href="https://arxiv.org/abs/2302.05733">Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks</a></li>
                    <li><a href="https://arxiv.org/abs/2311.17600">MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2308.03825">“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2312.04782">Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs</a></li>
                    <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2307.02483">Jailbroken: How Does LLM Safety Training Fail?</a></li>
                    <li><a href="https://arxiv.org/abs/2310.08419">Jailbreaking Black Box Large Language Models in Twenty Queries</a></li>
                    <li><a href="https://arxiv.org/abs/2010.15980">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts
                    </a></li>
                    <li><a href="https://arxiv.org/abs/2307.08715">MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots</a></li> 
                    <li><a href="https://arxiv.org/abs/2202.03286">Red Teaming Language Models with Language Models</a></li>
                </ul>
            <h4>Backdoor</h4>
                <ul>
                    <li><a href="https://arxiv.org/abs/2307.16888">Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection</a></li>
                    <li><a href="https://arxiv.org/html/2312.00374v1">Unleashing Cheapfakes through Trojan Plugins of Large Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2311.09433">“Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment</a></li>
                    <li><a href="https://arxiv.org/abs/2304.10638">Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning</a></li>
                    <li><a href="https://arxiv.org/abs/2311.17429">TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4</a></li>
                    <li><a href="https://arxiv.org/abs/2311.16194">BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP</a></li>
                    <li><a href="https://arxiv.org/abs/2312.00027">Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections</a></li>
                    <li><a href="https://arxiv.org/abs/2312.00374">The Philosopher's Stone: Trojaning Plugins of Large Language Models</a></li>
                    

                </ul>
            <h4>Others</h4>
                <ul>
                    <li><a href="https://arxiv.org/abs/2212.14315">"Real Attackers Don't Compute Gradients": Bridging the Gap Between Adversarial ML Research and Practice</a></li>
                    <li><a href="https://arxiv.org/abs/2310.11397">Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning</a></li>
                    <li><a href="https://arxiv.org/abs/2308.02816">Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models</a></li>
                    <li><a href="https://arxiv.org/abs/2312.04730">DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions</a></li>
                    <li><a href="https://arxiv.org/abs/2103.05633">Proof-of-Learning: Definitions and Practice</a></li>
                </ul>
        </section>
        <section id="defense">
            <h2>Defense</h2>
            <ul>
                <li><a href="https://dl.acm.org/doi/pdf/10.1145/3539618.3591945">Adversarial Meta Prompt Tuning for Open Compound Domain Adaptive Intent Detection</a></li>
                <li><a href="https://arxiv.org/abs/2308.02816">PromptCARE: Prompt Copyright Protection by Watermark Injection and Verificationn</a></li>
                <li><a href="https://arxiv.org/abs/2307.00691">From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy</a></li>
                <li><a href="https://arxiv.org/abs/2310.12860">Probing LLMs for hate speech detection: strengths and vulnerabilities</a></li>
                <li><a href="https://marketing.fmops.ai/blog/defending-llms-against-prompt-injection/">Defending LLMs against Prompt Injection</a></li>
                <li><a href="https://huggingface.co/jackhhao/jailbreak-classifier">jackhhao/jailbreak-classifier</a></li>
                <li><a href="https://www.nature.com/articles/s42256-023-00765-8">Defending ChatGPT against jailbreak attack via self-reminders</a></li>
                <li><a href="https://arxiv.org/abs/2306.08833">Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection</a></li>
                <li><a href="https://arxiv.org/abs/2309.07124">RAIN: Your Language Models Can Align Themselves without Finetuning</a></li>
                <li><a href="https://arxiv.org/abs/2312.10766">A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection</a></li>
                <li><a href="https://arxiv.org/abs/2310.03684">SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks</a></li>
                <li><a href="https://arxiv.org/abs/2203.06414">A Survey of Adversarial Defences and Robustness in NLP</a></li>
                <li><a href="https://arxiv.org/abs/2309.00614">Baseline Defenses for Adversarial Attacks Against Aligned Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2308.14132">Detecting Language Model Attacks with Perplexity</a></li> 
                <li><a href="https://arxiv.org/abs/2309.02705">Certifying LLM Safety against Adversarial Prompting</a></li> 

            </ul>
        </section>
        <section id="vulnerability-research">
            <h2>LLM Vulnerability Evaluation</h2>
            <ul>
                <li><a href="https://arxiv.org/abs/2308.10819">Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection</a></li>
                <li><a href="https://arxiv.org/abs/2310.09624">ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2309.02926">Demystifying RCE Vulnerabilities in LLM-Integrated Apps</a></li>
                <li><a href="https://arxiv.org/abs/2312.04724">Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2311.16153">Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications</a></li>
                <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
            </ul>
        </section>
        <section id="datasets">
            <h2>Datasets</h2>
            <ul>
                <li><a href="https://arxiv.org/abs/2311.16119">Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition</a></li>
                <li><a href="https://arxiv.org/abs/2311.18215">Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2311.01011">Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game</a></li>
                <li><a href="https://huggingface.co/datasets/deepset/prompt-injections">deepset/prompt-injections</a></li>

            </ul>
        </section>
    </main>
    <footer>
        <p>Follow me on:</p>
        <ul>
            <li><a href="https://x.com/7eHnpgIYyHE4iyG">X</a></li>
            <li><a href="https://qiita.com/tasuku-revol">Qita</a></li>
            <li><a href="https://note.com/tasty_zinnia174">Note</a></li>
        </ul>
        <p>Copyright ©Security of LLM Information Hub 2024</p>
    </footer>
</body>
</html>
