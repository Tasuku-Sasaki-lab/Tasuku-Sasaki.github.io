<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Security of LLM Information Hub</title>
    <link rel="stylesheet" href="styles.css">
    <script src="script.js" defer></script>
</head>
<body>
    <header>
        <h1>Security of LLM Information Hub</h1>
        <nav>
            <!-- <ul>
                <li><a href="#attack">Attack</a></li>
                <li><a href="#defense">Defense</a></li>
                <li><a href="#vulnerability-research">LLM Vulnerability Research</a></li>
                <li><a href="#datasets">Datasets</a></li>
            </ul>
        </nav> -->
        <!-- <button onclick="toggleLanguage()">日本語</button> -->
    </header>
    <main>
        <section id="attack">
            <h2>Attack</h2>
            <h4>Prompt Injection</h4>
            <ul>
                <li><a href="https://arxiv.org/abs/2306.05499">Prompt Injection attack against LLM-integrated Applications</a></li>
                <li><a href="https://arxiv.org/abs/2307.16888">Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection</a></li>
                <li><a href="https://arxiv.org/abs/2302.12173">Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</a></li>
                <li><a href="https://arxiv.org/abs/2211.09527">Ignore Previous Prompt: Attack Techniques For Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2306.08833">Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection</a></li>

            </ul>
        </section>
        <section id="defense">
            <h2>Defense</h2>
            <ul>
                <li><a href="link-to-paper">Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection</a></li>
            </ul>
        </section>
        <section id="vulnerability-research">
            <h2>LLM Vulnerability Research</h2>
            <ul>
                <li><a href="link-to-paper">Compromising LLMs: The Advent of AI Malware</a></li>
                <li><a href="link-to-paper">Generative AI’s Biggest Security Flaw Is Not Easy to Fix</a></li>
            </ul>
        </section>
        <section id="datasets">
            <h2>Datasets</h2>
            <ul>
                <li><a href="link-to-paper">Virtual Prompt Injection for Instruction-Tuned Large Language Models</a></li>
            </ul>
        </section>
    </main>
    <footer>
        <p>Follow us on:</p>
        <ul>
            <li><a href="https://qiita.com">Qiita</a></li>
            <li><a href="https://www.facebook.com">Facebook</a></li>
            <li><a href="https://note.com">Note</a></li>
        </ul>
        <p>Copyright © Your Website 2023</p>
    </footer>
</body>
</html>
