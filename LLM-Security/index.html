<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Security of LLM Information Hub</title>
    <link rel="stylesheet" href="styles.css">
    <script src="script.js" defer></script>
</head>
<body>
    <header>
        <h1>Security of LLM Information Hub</h1>
        <nav>
            <h1>Welcome to Our Website!</h1>
            <p>For inquiries, requests, or job proposals, please DM <a href = "https://x.com/7eHnpgIYyHE4iyG">X</a></p>
            
            <p>We are looking for friends to help build the site.</p>
            <!-- <ul>
                <li><a href="#attack">Attack</a></li>
                <li><a href="#defense">Defense</a></li>
                <li><a href="#vulnerability-research">LLM Vulnerability Research</a></li>
                <li><a href="#datasets">Datasets</a></li>
            </ul>
        </nav> -->
        <!-- <button onclick="toggleLanguage()">日本語</button> -->
    </header>
    <main>
        <section id="attack">
            <h1>For Beginners</h1>
                <!-- <h4>Attack</h4>
                <h4>Defence</h4>
                <h4>Others</h4>
                <h4>Famous X accounts</h4> -->
                <p>Currently, only the Japanese version is available. The English version is coming soon.</p>
                <iframe src="https://docs.google.com/presentation/d/1C1FJBUTDP100g56nicNUK9w_P3LIFeEc/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
                <iframe src="https://www.slideshare.net/slideshow/embed_code/key/4P7tlLFTSj2zgl?startSlide=1" width="960" height="569" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px;max-width: 100%;" allowfullscreen></iframe><div style="margin-bottom:5px"><strong><a href="https://www.slideshare.net/slideshow/llm-llm/273266619" title="LLMセキュリティの概要　　LLM時代のセキュリティリスク~研究最前線と実務への展開~ 【ハイブリッド開催】" target="_blank">LLMセキュリティの概要　　LLM時代のセキュリティリスク~研究最前線と実務への展開~ 【ハイブリッド開催】</a></strong> from <strong><a href="https://www.slideshare.net/tsasakirevol" target="_blank">tsasakirevol</a></strong></div>
            <h1>Informartion Hub</h1>
            <h2>Survey</h2>
            <ul>
                <li><a href="https://arxiv.org/abs/2401.05561">TrustLLM: Trustworthiness in Large Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2403.04786v2">Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2312.02003">A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly</a></li>
            </ul>
            <h2>Attack</h2>
            <h4>Prompt Injection</h4>
            <ul>
                <li><a href="https://arxiv.org/abs/2306.05499">Prompt Injection attack against LLM-integrated Applications</a></li>
                <li><a href="https://arxiv.org/abs/2302.12173">Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</a></li>
                <li><a href="https://arxiv.org/abs/2211.09527">Ignore Previous Prompt: Attack Techniques For Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2306.08833">Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection</a></li>
                <li><a href="https://arxiv.org/abs/2203.10714">A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement</a></li>
                <li><a href="https://arxiv.org/abs/2302.04237">Black Box Adversarial Prompting for Foundation Models</a></li>
                <li><a href="https://aclanthology.org/2022.acl-long.174/">Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis</a></li>
                <li><a href="https://arxiv.org/pdf/2206.11349">Prompt Injection: Parameterization of Fixed Inputs</a></li>
                <li><a href="https://arxiv.org/abs/2310.12815">Prompt Injection Attacks and Defenses in LLM-Integrated Applications</a></li>
                <li><a href="https://arxiv.org/abs/2308.01990">From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?</a></li>
                <li><a href="https://arxiv.org/abs/2403.17710">Optimization-based Prompt Injection Attack to LLM-as-a-Judge</a></li>
            </ul>
            <h4>Jailbreak</h4>
                <ul>
                    <li><a href="https://www.digitaltrends.com/computing/how-to-jailbreak-chatgpt/">How to jailbreak ChatGPT: get it to really do what you want</a></li>
                    <li><a href="https://arxiv.org/abs/2302.05733">Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks</a></li>
                    <li><a href="https://arxiv.org/abs/2311.17600">MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2312.04782">Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs</a></li>
                    <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2307.02483">Jailbroken: How Does LLM Safety Training Fail?</a></li>
                    <li><a href="https://arxiv.org/abs/2310.08419">Jailbreaking Black Box Large Language Models in Twenty Queries</a></li>
                    <li><a href="https://arxiv.org/abs/2010.15980">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts
                    </a></li>
                    <li><a href="https://arxiv.org/abs/2307.08715">MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots</a></li> 
                    <li><a href="https://arxiv.org/abs/2202.03286">Red Teaming Language Models with Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2402.03299">GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2404.04392">Increased LLM Vulnerabilities from Fine-tuning and Quantization</a></li> 
                    <li><a href="https://arxiv.org/abs/2401.09798">All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks</a></li>
                </ul>
            <h4>Backdoor</h4>
                <ul>
                    <li><a href="https://arxiv.org/abs/2307.16888">Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection</a></li>
                    <li><a href="https://arxiv.org/abs/2310.03693">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a></li>
                    <li><a href="https://arxiv.org/html/2312.00374v1">Unleashing Cheapfakes through Trojan Plugins of Large Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2311.09433">“Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment</a></li>
                    <li><a href="https://arxiv.org/abs/2304.10638">Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning</a></li>
                    <li><a href="https://arxiv.org/abs/2311.17429">TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4</a></li>
                    <li><a href="https://arxiv.org/abs/2311.16194">BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP</a></li>
                    <li><a href="https://arxiv.org/abs/2312.00027">Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections</a></li>
                    <li><a href="https://arxiv.org/abs/2312.00374">The Philosopher's Stone: Trojaning Plugins of Large Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2401.05566">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</a></li>
                    

                </ul>
            <h4>RAG</h4>
            <h4>Multi-Modal</h4>
            <h4>Others</h4>
                <ul>
                    <li><a href="https://arxiv.org/abs/2212.14315">"Real Attackers Don't Compute Gradients": Bridging the Gap Between Adversarial ML Research and Practice</a></li>
                    <li><a href="https://arxiv.org/abs/2310.11397">Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning</a></li>
                    <li><a href="https://arxiv.org/abs/2308.02816">Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models</a></li>
                    <li><a href="https://arxiv.org/abs/2312.04730">DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions</a></li>
                    <li><a href="https://arxiv.org/abs/2103.05633">Proof-of-Learning: Definitions and Practice</a></li>
                </ul>
        </section>
        <section id="defense">
            <h2>Defense</h2>
            <ul>
                <li><a href="https://dl.acm.org/doi/pdf/10.1145/3539618.3591945">Adversarial Meta Prompt Tuning for Open Compound Domain Adaptive Intent Detection</a></li>
                <li><a href="https://arxiv.org/abs/2308.02816">PromptCARE: Prompt Copyright Protection by Watermark Injection and Verificationn</a></li>
                <li><a href="https://arxiv.org/abs/2307.00691">From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy</a></li>
                <li><a href="https://arxiv.org/abs/2310.12860">Probing LLMs for hate speech detection: strengths and vulnerabilities</a></li>
                <li><a href="https://marketing.fmops.ai/blog/defending-llms-against-prompt-injection/">Defending LLMs against Prompt Injection</a></li>
                <li><a href="https://huggingface.co/jackhhao/jailbreak-classifier">jackhhao/jailbreak-classifier</a></li>
                <li><a href="https://www.nature.com/articles/s42256-023-00765-8">Defending ChatGPT against jailbreak attack via self-reminders</a></li>
                <li><a href="https://arxiv.org/abs/2306.08833">Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection</a></li>
                <li><a href="https://arxiv.org/abs/2309.07124">RAIN: Your Language Models Can Align Themselves without Finetuning</a></li>
                <li><a href="https://arxiv.org/abs/2312.10766">A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection</a></li>
                <li><a href="https://arxiv.org/abs/2310.03684">SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks</a></li>
                <li><a href="https://arxiv.org/abs/2203.06414">A Survey of Adversarial Defences and Robustness in NLP</a></li>
                <li><a href="https://arxiv.org/abs/2309.00614">Baseline Defenses for Adversarial Attacks Against Aligned Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2308.14132">Detecting Language Model Attacks with Perplexity</a></li> 
                <li><a href="https://arxiv.org/abs/2309.02705">Certifying LLM Safety against Adversarial Prompting</a></li>
                <li><a href="https://arxiv.org/abs/2310.19737">Adversarial Attacks and Defenses in Large Language Models: Old and New Threats</a></li>
                <li><a href="https://arxiv.org/abs/2401.06561">Intention Analysis Makes LLMs A Good Jailbreak Defender</a></li>
                <li><a href="https://arxiv.org/abs/2311.07689">MART: Improving LLM Safety with Multi-round Automatic Red-Teaming</a></li>
                <li><a href="https://arxiv.org/abs/2401.06373">How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs</a></li>
                <li><a href="https://arxiv.org/abs/2402.06900">Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric</a></li>
                <li><a href="https://arxiv.org/abs/2402.08983">SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding</a></li>
                <li><a href="https://arxiv.org/abs/2402.16192">Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing</a></li>
                <li><a href="https://arxiv.org/abs/2401.17263">Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks</a></li>
                <li><a href="https://aclanthology.org/2022.naacl-main.59/">Explaining Toxic Text via Knowledge Enhanced Text Generation</a></li>
                <li><a href="https://aclanthology.org/2022.findings-acl.176/">Your fairness may vary: Pretrained language model fairness in toxic text classification</a></li>
                <li><a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a></li>

            </ul>
        </section>
        <section id="vulnerability-research">
            <h2>LLM Vulnerability Evaluation</h2>
            <ul>
                <li><a href="https://arxiv.org/abs/2308.10819">Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection</a></li>
                <li><a href="https://arxiv.org/abs/2310.09624">ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2309.02926">Demystifying RCE Vulnerabilities in LLM-Integrated Apps</a></li>
                <li><a href="https://arxiv.org/abs/2312.04724">Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2311.16153">Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications</a></li>
                <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2401.09002">AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2402.04249">HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal</a></li>
            </ul>
        </section>
        <section id="datasets">
            <h2>Datasets</h2>
            <ul>
                <li><a href="https://arxiv.org/abs/2311.16119">Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition</a></li>
                <li><a href="https://arxiv.org/abs/2311.18215">Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2311.01011">Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game</a></li>
                <li><a href="https://huggingface.co/datasets/deepset/prompt-injections">deepset/prompt-injections</a></li>
                <li><a href="https://github.com/Libr-AI/do-not-answer">Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs</a></li>
                <li><a href="https://liat-aip.sakura.ne.jp/wp/answercarefully-dataset/">AnswerCarefully Dataset</a></li>
                <li><a href="https://arxiv.org/abs/2308.03825">“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</a></li>

            </ul>
        </section>
    </main>
    <footer>
        <p>Follow me on:</p>
        <ul>
            <li><a href="https://x.com/7eHnpgIYyHE4iyG">X</a></li>
            <li><a href="https://qiita.com/tasuku-revol">Qita</a></li>
        </ul>
        <p>Copyright ©Security of LLM Information Hub 2024</p>
    </footer>
</body>
</html>
